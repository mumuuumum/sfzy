{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.4598540145985401,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.014598540145985401,
      "grad_norm": 0.474153995513916,
      "learning_rate": 4.854368932038835e-06,
      "loss": 1.6011,
      "step": 10
    },
    {
      "epoch": 0.029197080291970802,
      "grad_norm": 0.47010478377342224,
      "learning_rate": 9.70873786407767e-06,
      "loss": 1.5736,
      "step": 20
    },
    {
      "epoch": 0.043795620437956206,
      "grad_norm": 0.521587073802948,
      "learning_rate": 1.4563106796116505e-05,
      "loss": 1.5892,
      "step": 30
    },
    {
      "epoch": 0.058394160583941604,
      "grad_norm": 0.689390242099762,
      "learning_rate": 1.8932038834951457e-05,
      "loss": 1.4888,
      "step": 40
    },
    {
      "epoch": 0.072992700729927,
      "grad_norm": 0.6157707571983337,
      "learning_rate": 2.3786407766990294e-05,
      "loss": 1.3035,
      "step": 50
    },
    {
      "epoch": 0.08759124087591241,
      "grad_norm": 0.6498854160308838,
      "learning_rate": 2.8640776699029125e-05,
      "loss": 1.1974,
      "step": 60
    },
    {
      "epoch": 0.10218978102189781,
      "grad_norm": 0.594110906124115,
      "learning_rate": 3.3495145631067966e-05,
      "loss": 0.984,
      "step": 70
    },
    {
      "epoch": 0.11678832116788321,
      "grad_norm": 0.692129909992218,
      "learning_rate": 3.83495145631068e-05,
      "loss": 0.9348,
      "step": 80
    },
    {
      "epoch": 0.13138686131386862,
      "grad_norm": 0.5947320461273193,
      "learning_rate": 4.3203883495145634e-05,
      "loss": 0.876,
      "step": 90
    },
    {
      "epoch": 0.145985401459854,
      "grad_norm": 0.558393120765686,
      "learning_rate": 4.805825242718447e-05,
      "loss": 0.8186,
      "step": 100
    },
    {
      "epoch": 0.16058394160583941,
      "grad_norm": 0.5756039023399353,
      "learning_rate": 5.29126213592233e-05,
      "loss": 0.7235,
      "step": 110
    },
    {
      "epoch": 0.17518248175182483,
      "grad_norm": 0.5095035433769226,
      "learning_rate": 5.7766990291262135e-05,
      "loss": 0.6742,
      "step": 120
    },
    {
      "epoch": 0.1897810218978102,
      "grad_norm": 0.6229369640350342,
      "learning_rate": 6.262135922330098e-05,
      "loss": 0.6809,
      "step": 130
    },
    {
      "epoch": 0.20437956204379562,
      "grad_norm": 0.6370835900306702,
      "learning_rate": 6.747572815533982e-05,
      "loss": 0.6938,
      "step": 140
    },
    {
      "epoch": 0.21897810218978103,
      "grad_norm": 0.5314831137657166,
      "learning_rate": 7.233009708737864e-05,
      "loss": 0.6704,
      "step": 150
    },
    {
      "epoch": 0.23357664233576642,
      "grad_norm": 0.6334070563316345,
      "learning_rate": 7.718446601941748e-05,
      "loss": 0.6334,
      "step": 160
    },
    {
      "epoch": 0.24817518248175183,
      "grad_norm": 0.6747769117355347,
      "learning_rate": 8.203883495145631e-05,
      "loss": 0.6424,
      "step": 170
    },
    {
      "epoch": 0.26277372262773724,
      "grad_norm": 0.6511096358299255,
      "learning_rate": 8.689320388349514e-05,
      "loss": 0.6378,
      "step": 180
    },
    {
      "epoch": 0.2773722627737226,
      "grad_norm": 0.8587298393249512,
      "learning_rate": 9.174757281553399e-05,
      "loss": 0.5863,
      "step": 190
    },
    {
      "epoch": 0.291970802919708,
      "grad_norm": 0.659281313419342,
      "learning_rate": 9.660194174757282e-05,
      "loss": 0.5792,
      "step": 200
    },
    {
      "epoch": 0.30656934306569344,
      "grad_norm": 0.9270609617233276,
      "learning_rate": 9.999935045760715e-05,
      "loss": 0.5764,
      "step": 210
    },
    {
      "epoch": 0.32116788321167883,
      "grad_norm": 0.7036126852035522,
      "learning_rate": 9.998780350676213e-05,
      "loss": 0.5707,
      "step": 220
    },
    {
      "epoch": 0.3357664233576642,
      "grad_norm": 0.79387366771698,
      "learning_rate": 9.996182611738734e-05,
      "loss": 0.6292,
      "step": 230
    },
    {
      "epoch": 0.35036496350364965,
      "grad_norm": 0.7426979541778564,
      "learning_rate": 9.992142578861446e-05,
      "loss": 0.5579,
      "step": 240
    },
    {
      "epoch": 0.36496350364963503,
      "grad_norm": 0.8333824276924133,
      "learning_rate": 9.986661418317759e-05,
      "loss": 0.5452,
      "step": 250
    },
    {
      "epoch": 0.3795620437956204,
      "grad_norm": 0.7631967067718506,
      "learning_rate": 9.97974071240465e-05,
      "loss": 0.5348,
      "step": 260
    },
    {
      "epoch": 0.39416058394160586,
      "grad_norm": 0.8771794438362122,
      "learning_rate": 9.971382458985882e-05,
      "loss": 0.5703,
      "step": 270
    },
    {
      "epoch": 0.40875912408759124,
      "grad_norm": 0.7312816381454468,
      "learning_rate": 9.961589070915268e-05,
      "loss": 0.5307,
      "step": 280
    },
    {
      "epoch": 0.4233576642335766,
      "grad_norm": 0.8898593187332153,
      "learning_rate": 9.95036337534012e-05,
      "loss": 0.5536,
      "step": 290
    },
    {
      "epoch": 0.43795620437956206,
      "grad_norm": 0.7912207841873169,
      "learning_rate": 9.937708612885126e-05,
      "loss": 0.5279,
      "step": 300
    },
    {
      "epoch": 0.45255474452554745,
      "grad_norm": 0.7731040716171265,
      "learning_rate": 9.923628436716836e-05,
      "loss": 0.4857,
      "step": 310
    },
    {
      "epoch": 0.46715328467153283,
      "grad_norm": 0.662590742111206,
      "learning_rate": 9.908126911489073e-05,
      "loss": 0.5064,
      "step": 320
    },
    {
      "epoch": 0.48175182481751827,
      "grad_norm": 0.6857175230979919,
      "learning_rate": 9.891208512169554e-05,
      "loss": 0.5207,
      "step": 330
    },
    {
      "epoch": 0.49635036496350365,
      "grad_norm": 0.8782424926757812,
      "learning_rate": 9.872878122748058e-05,
      "loss": 0.5195,
      "step": 340
    },
    {
      "epoch": 0.5109489051094891,
      "grad_norm": 0.8150352835655212,
      "learning_rate": 9.853141034826522e-05,
      "loss": 0.5425,
      "step": 350
    },
    {
      "epoch": 0.5255474452554745,
      "grad_norm": 0.8332362771034241,
      "learning_rate": 9.832002946091459e-05,
      "loss": 0.534,
      "step": 360
    },
    {
      "epoch": 0.5401459854014599,
      "grad_norm": 0.792351484298706,
      "learning_rate": 9.809469958669164e-05,
      "loss": 0.4862,
      "step": 370
    },
    {
      "epoch": 0.5547445255474452,
      "grad_norm": 0.9537925124168396,
      "learning_rate": 9.785548577364152e-05,
      "loss": 0.493,
      "step": 380
    },
    {
      "epoch": 0.5693430656934306,
      "grad_norm": 1.064953327178955,
      "learning_rate": 9.760245707781355e-05,
      "loss": 0.5066,
      "step": 390
    },
    {
      "epoch": 0.583941605839416,
      "grad_norm": 0.8237999081611633,
      "learning_rate": 9.733568654332618e-05,
      "loss": 0.4837,
      "step": 400
    },
    {
      "epoch": 0.5985401459854015,
      "grad_norm": 0.7828624248504639,
      "learning_rate": 9.705525118128073e-05,
      "loss": 0.4947,
      "step": 410
    },
    {
      "epoch": 0.6131386861313869,
      "grad_norm": 0.911079466342926,
      "learning_rate": 9.67612319475298e-05,
      "loss": 0.4785,
      "step": 420
    },
    {
      "epoch": 0.6277372262773723,
      "grad_norm": 0.7735104560852051,
      "learning_rate": 9.645371371930709e-05,
      "loss": 0.4984,
      "step": 430
    },
    {
      "epoch": 0.6423357664233577,
      "grad_norm": 0.8236644864082336,
      "learning_rate": 9.613278527072504e-05,
      "loss": 0.4813,
      "step": 440
    },
    {
      "epoch": 0.656934306569343,
      "grad_norm": 0.800072193145752,
      "learning_rate": 9.579853924714759e-05,
      "loss": 0.5068,
      "step": 450
    },
    {
      "epoch": 0.6715328467153284,
      "grad_norm": 0.8278487324714661,
      "learning_rate": 9.545107213844546e-05,
      "loss": 0.4626,
      "step": 460
    },
    {
      "epoch": 0.6861313868613139,
      "grad_norm": 0.8579379320144653,
      "learning_rate": 9.509048425114148e-05,
      "loss": 0.5015,
      "step": 470
    },
    {
      "epoch": 0.7007299270072993,
      "grad_norm": 0.9362450838088989,
      "learning_rate": 9.471687967945416e-05,
      "loss": 0.4784,
      "step": 480
    },
    {
      "epoch": 0.7153284671532847,
      "grad_norm": 0.9878629446029663,
      "learning_rate": 9.433036627524794e-05,
      "loss": 0.4683,
      "step": 490
    },
    {
      "epoch": 0.7299270072992701,
      "grad_norm": 0.8304235935211182,
      "learning_rate": 9.39310556168985e-05,
      "loss": 0.5026,
      "step": 500
    },
    {
      "epoch": 0.7445255474452555,
      "grad_norm": 0.8765249848365784,
      "learning_rate": 9.35190629770825e-05,
      "loss": 0.5117,
      "step": 510
    },
    {
      "epoch": 0.7591240875912408,
      "grad_norm": 0.8345662355422974,
      "learning_rate": 9.309450728950069e-05,
      "loss": 0.4673,
      "step": 520
    },
    {
      "epoch": 0.7737226277372263,
      "grad_norm": 0.8436962962150574,
      "learning_rate": 9.265751111454425e-05,
      "loss": 0.5026,
      "step": 530
    },
    {
      "epoch": 0.7883211678832117,
      "grad_norm": 0.8265005350112915,
      "learning_rate": 9.220820060391416e-05,
      "loss": 0.4496,
      "step": 540
    },
    {
      "epoch": 0.8029197080291971,
      "grad_norm": 1.0502997636795044,
      "learning_rate": 9.174670546420379e-05,
      "loss": 0.53,
      "step": 550
    },
    {
      "epoch": 0.8175182481751825,
      "grad_norm": 0.8298702836036682,
      "learning_rate": 9.12731589194554e-05,
      "loss": 0.4508,
      "step": 560
    },
    {
      "epoch": 0.8321167883211679,
      "grad_norm": 0.8837051391601562,
      "learning_rate": 9.078769767270106e-05,
      "loss": 0.4592,
      "step": 570
    },
    {
      "epoch": 0.8467153284671532,
      "grad_norm": 0.9696693420410156,
      "learning_rate": 9.029046186649927e-05,
      "loss": 0.4614,
      "step": 580
    },
    {
      "epoch": 0.8613138686131386,
      "grad_norm": 0.8426806330680847,
      "learning_rate": 8.97815950424789e-05,
      "loss": 0.4691,
      "step": 590
    },
    {
      "epoch": 0.8759124087591241,
      "grad_norm": 0.8292596340179443,
      "learning_rate": 8.926124409990149e-05,
      "loss": 0.4801,
      "step": 600
    },
    {
      "epoch": 0.8905109489051095,
      "grad_norm": 0.814528226852417,
      "learning_rate": 8.87295592532547e-05,
      "loss": 0.479,
      "step": 610
    },
    {
      "epoch": 0.9051094890510949,
      "grad_norm": 0.9417440295219421,
      "learning_rate": 8.818669398888834e-05,
      "loss": 0.4743,
      "step": 620
    },
    {
      "epoch": 0.9197080291970803,
      "grad_norm": 0.8523961305618286,
      "learning_rate": 8.763280502070617e-05,
      "loss": 0.4547,
      "step": 630
    },
    {
      "epoch": 0.9343065693430657,
      "grad_norm": 1.0132845640182495,
      "learning_rate": 8.706805224492582e-05,
      "loss": 0.4656,
      "step": 640
    },
    {
      "epoch": 0.948905109489051,
      "grad_norm": 0.888124942779541,
      "learning_rate": 8.649259869392004e-05,
      "loss": 0.474,
      "step": 650
    },
    {
      "epoch": 0.9635036496350365,
      "grad_norm": 0.8103448748588562,
      "learning_rate": 8.590661048915272e-05,
      "loss": 0.451,
      "step": 660
    },
    {
      "epoch": 0.9781021897810219,
      "grad_norm": 0.8937534093856812,
      "learning_rate": 8.531025679322302e-05,
      "loss": 0.4534,
      "step": 670
    },
    {
      "epoch": 0.9927007299270073,
      "grad_norm": 0.837407112121582,
      "learning_rate": 8.47037097610317e-05,
      "loss": 0.4627,
      "step": 680
    },
    {
      "epoch": 1.0072992700729928,
      "grad_norm": 0.8500024080276489,
      "learning_rate": 8.408714449008352e-05,
      "loss": 0.4142,
      "step": 690
    },
    {
      "epoch": 1.0218978102189782,
      "grad_norm": 1.029523491859436,
      "learning_rate": 8.346073896994031e-05,
      "loss": 0.4142,
      "step": 700
    },
    {
      "epoch": 1.0364963503649636,
      "grad_norm": 0.9407436847686768,
      "learning_rate": 8.282467403083903e-05,
      "loss": 0.4066,
      "step": 710
    },
    {
      "epoch": 1.051094890510949,
      "grad_norm": 0.8774054050445557,
      "learning_rate": 8.217913329148981e-05,
      "loss": 0.3995,
      "step": 720
    },
    {
      "epoch": 1.0656934306569343,
      "grad_norm": 0.9214247465133667,
      "learning_rate": 8.152430310606922e-05,
      "loss": 0.382,
      "step": 730
    },
    {
      "epoch": 1.0802919708029197,
      "grad_norm": 0.8537412881851196,
      "learning_rate": 8.086037251042348e-05,
      "loss": 0.406,
      "step": 740
    },
    {
      "epoch": 1.094890510948905,
      "grad_norm": 0.9219521880149841,
      "learning_rate": 8.018753316749786e-05,
      "loss": 0.3717,
      "step": 750
    },
    {
      "epoch": 1.1094890510948905,
      "grad_norm": 0.8020859360694885,
      "learning_rate": 7.95059793120076e-05,
      "loss": 0.3847,
      "step": 760
    },
    {
      "epoch": 1.1240875912408759,
      "grad_norm": 0.976387619972229,
      "learning_rate": 7.881590769436634e-05,
      "loss": 0.4179,
      "step": 770
    },
    {
      "epoch": 1.1386861313868613,
      "grad_norm": 0.8144912719726562,
      "learning_rate": 7.811751752388834e-05,
      "loss": 0.3979,
      "step": 780
    },
    {
      "epoch": 1.1532846715328466,
      "grad_norm": 0.8935455083847046,
      "learning_rate": 7.741101041128099e-05,
      "loss": 0.4092,
      "step": 790
    },
    {
      "epoch": 1.167883211678832,
      "grad_norm": 0.993914008140564,
      "learning_rate": 7.669659031044392e-05,
      "loss": 0.4286,
      "step": 800
    },
    {
      "epoch": 1.1824817518248176,
      "grad_norm": 0.9246808886528015,
      "learning_rate": 7.597446345959178e-05,
      "loss": 0.3924,
      "step": 810
    },
    {
      "epoch": 1.197080291970803,
      "grad_norm": 0.9731178283691406,
      "learning_rate": 7.524483832171756e-05,
      "loss": 0.4207,
      "step": 820
    },
    {
      "epoch": 1.2116788321167884,
      "grad_norm": 0.9542856216430664,
      "learning_rate": 7.450792552441364e-05,
      "loss": 0.4077,
      "step": 830
    },
    {
      "epoch": 1.2262773722627738,
      "grad_norm": 0.9903732538223267,
      "learning_rate": 7.376393779906806e-05,
      "loss": 0.4086,
      "step": 840
    },
    {
      "epoch": 1.2408759124087592,
      "grad_norm": 0.9735646843910217,
      "learning_rate": 7.301308991945329e-05,
      "loss": 0.4207,
      "step": 850
    },
    {
      "epoch": 1.2554744525547445,
      "grad_norm": 0.9861911535263062,
      "learning_rate": 7.225559863972555e-05,
      "loss": 0.3944,
      "step": 860
    },
    {
      "epoch": 1.27007299270073,
      "grad_norm": 0.9942032694816589,
      "learning_rate": 7.149168263185249e-05,
      "loss": 0.4058,
      "step": 870
    },
    {
      "epoch": 1.2846715328467153,
      "grad_norm": 1.0354939699172974,
      "learning_rate": 7.0721562422487e-05,
      "loss": 0.4286,
      "step": 880
    },
    {
      "epoch": 1.2992700729927007,
      "grad_norm": 1.002482533454895,
      "learning_rate": 6.994546032930582e-05,
      "loss": 0.3883,
      "step": 890
    },
    {
      "epoch": 1.313868613138686,
      "grad_norm": 0.945805013179779,
      "learning_rate": 6.916360039683109e-05,
      "loss": 0.3987,
      "step": 900
    },
    {
      "epoch": 1.3284671532846715,
      "grad_norm": 0.9589716196060181,
      "learning_rate": 6.83762083317533e-05,
      "loss": 0.3725,
      "step": 910
    },
    {
      "epoch": 1.343065693430657,
      "grad_norm": 0.9340313076972961,
      "learning_rate": 6.758351143777449e-05,
      "loss": 0.4015,
      "step": 920
    },
    {
      "epoch": 1.3576642335766422,
      "grad_norm": 0.9945915937423706,
      "learning_rate": 6.678573854999038e-05,
      "loss": 0.4252,
      "step": 930
    },
    {
      "epoch": 1.3722627737226278,
      "grad_norm": 0.9992026686668396,
      "learning_rate": 6.598311996883049e-05,
      "loss": 0.4389,
      "step": 940
    },
    {
      "epoch": 1.3868613138686132,
      "grad_norm": 0.9854737520217896,
      "learning_rate": 6.517588739357518e-05,
      "loss": 0.4146,
      "step": 950
    },
    {
      "epoch": 1.4014598540145986,
      "grad_norm": 0.9030470848083496,
      "learning_rate": 6.436427385546883e-05,
      "loss": 0.4023,
      "step": 960
    },
    {
      "epoch": 1.416058394160584,
      "grad_norm": 1.0019944906234741,
      "learning_rate": 6.354851365044868e-05,
      "loss": 0.4191,
      "step": 970
    },
    {
      "epoch": 1.4306569343065694,
      "grad_norm": 0.9242604374885559,
      "learning_rate": 6.272884227150834e-05,
      "loss": 0.3874,
      "step": 980
    },
    {
      "epoch": 1.4452554744525548,
      "grad_norm": 0.9749046564102173,
      "learning_rate": 6.1905496340716e-05,
      "loss": 0.401,
      "step": 990
    },
    {
      "epoch": 1.4598540145985401,
      "grad_norm": 0.9817001223564148,
      "learning_rate": 6.107871354090643e-05,
      "loss": 0.4028,
      "step": 1000
    }
  ],
  "logging_steps": 10,
  "max_steps": 2055,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.1754913730637332e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
