{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 2055,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.014598540145985401,
      "grad_norm": 0.474153995513916,
      "learning_rate": 4.854368932038835e-06,
      "loss": 1.6011,
      "step": 10
    },
    {
      "epoch": 0.029197080291970802,
      "grad_norm": 0.47010478377342224,
      "learning_rate": 9.70873786407767e-06,
      "loss": 1.5736,
      "step": 20
    },
    {
      "epoch": 0.043795620437956206,
      "grad_norm": 0.521587073802948,
      "learning_rate": 1.4563106796116505e-05,
      "loss": 1.5892,
      "step": 30
    },
    {
      "epoch": 0.058394160583941604,
      "grad_norm": 0.689390242099762,
      "learning_rate": 1.8932038834951457e-05,
      "loss": 1.4888,
      "step": 40
    },
    {
      "epoch": 0.072992700729927,
      "grad_norm": 0.6157707571983337,
      "learning_rate": 2.3786407766990294e-05,
      "loss": 1.3035,
      "step": 50
    },
    {
      "epoch": 0.08759124087591241,
      "grad_norm": 0.6498854160308838,
      "learning_rate": 2.8640776699029125e-05,
      "loss": 1.1974,
      "step": 60
    },
    {
      "epoch": 0.10218978102189781,
      "grad_norm": 0.594110906124115,
      "learning_rate": 3.3495145631067966e-05,
      "loss": 0.984,
      "step": 70
    },
    {
      "epoch": 0.11678832116788321,
      "grad_norm": 0.692129909992218,
      "learning_rate": 3.83495145631068e-05,
      "loss": 0.9348,
      "step": 80
    },
    {
      "epoch": 0.13138686131386862,
      "grad_norm": 0.5947320461273193,
      "learning_rate": 4.3203883495145634e-05,
      "loss": 0.876,
      "step": 90
    },
    {
      "epoch": 0.145985401459854,
      "grad_norm": 0.558393120765686,
      "learning_rate": 4.805825242718447e-05,
      "loss": 0.8186,
      "step": 100
    },
    {
      "epoch": 0.16058394160583941,
      "grad_norm": 0.5756039023399353,
      "learning_rate": 5.29126213592233e-05,
      "loss": 0.7235,
      "step": 110
    },
    {
      "epoch": 0.17518248175182483,
      "grad_norm": 0.5095035433769226,
      "learning_rate": 5.7766990291262135e-05,
      "loss": 0.6742,
      "step": 120
    },
    {
      "epoch": 0.1897810218978102,
      "grad_norm": 0.6229369640350342,
      "learning_rate": 6.262135922330098e-05,
      "loss": 0.6809,
      "step": 130
    },
    {
      "epoch": 0.20437956204379562,
      "grad_norm": 0.6370835900306702,
      "learning_rate": 6.747572815533982e-05,
      "loss": 0.6938,
      "step": 140
    },
    {
      "epoch": 0.21897810218978103,
      "grad_norm": 0.5314831137657166,
      "learning_rate": 7.233009708737864e-05,
      "loss": 0.6704,
      "step": 150
    },
    {
      "epoch": 0.23357664233576642,
      "grad_norm": 0.6334070563316345,
      "learning_rate": 7.718446601941748e-05,
      "loss": 0.6334,
      "step": 160
    },
    {
      "epoch": 0.24817518248175183,
      "grad_norm": 0.6747769117355347,
      "learning_rate": 8.203883495145631e-05,
      "loss": 0.6424,
      "step": 170
    },
    {
      "epoch": 0.26277372262773724,
      "grad_norm": 0.6511096358299255,
      "learning_rate": 8.689320388349514e-05,
      "loss": 0.6378,
      "step": 180
    },
    {
      "epoch": 0.2773722627737226,
      "grad_norm": 0.8587298393249512,
      "learning_rate": 9.174757281553399e-05,
      "loss": 0.5863,
      "step": 190
    },
    {
      "epoch": 0.291970802919708,
      "grad_norm": 0.659281313419342,
      "learning_rate": 9.660194174757282e-05,
      "loss": 0.5792,
      "step": 200
    },
    {
      "epoch": 0.30656934306569344,
      "grad_norm": 0.9270609617233276,
      "learning_rate": 9.999935045760715e-05,
      "loss": 0.5764,
      "step": 210
    },
    {
      "epoch": 0.32116788321167883,
      "grad_norm": 0.7036126852035522,
      "learning_rate": 9.998780350676213e-05,
      "loss": 0.5707,
      "step": 220
    },
    {
      "epoch": 0.3357664233576642,
      "grad_norm": 0.79387366771698,
      "learning_rate": 9.996182611738734e-05,
      "loss": 0.6292,
      "step": 230
    },
    {
      "epoch": 0.35036496350364965,
      "grad_norm": 0.7426979541778564,
      "learning_rate": 9.992142578861446e-05,
      "loss": 0.5579,
      "step": 240
    },
    {
      "epoch": 0.36496350364963503,
      "grad_norm": 0.8333824276924133,
      "learning_rate": 9.986661418317759e-05,
      "loss": 0.5452,
      "step": 250
    },
    {
      "epoch": 0.3795620437956204,
      "grad_norm": 0.7631967067718506,
      "learning_rate": 9.97974071240465e-05,
      "loss": 0.5348,
      "step": 260
    },
    {
      "epoch": 0.39416058394160586,
      "grad_norm": 0.8771794438362122,
      "learning_rate": 9.971382458985882e-05,
      "loss": 0.5703,
      "step": 270
    },
    {
      "epoch": 0.40875912408759124,
      "grad_norm": 0.7312816381454468,
      "learning_rate": 9.961589070915268e-05,
      "loss": 0.5307,
      "step": 280
    },
    {
      "epoch": 0.4233576642335766,
      "grad_norm": 0.8898593187332153,
      "learning_rate": 9.95036337534012e-05,
      "loss": 0.5536,
      "step": 290
    },
    {
      "epoch": 0.43795620437956206,
      "grad_norm": 0.7912207841873169,
      "learning_rate": 9.937708612885126e-05,
      "loss": 0.5279,
      "step": 300
    },
    {
      "epoch": 0.45255474452554745,
      "grad_norm": 0.7731040716171265,
      "learning_rate": 9.923628436716836e-05,
      "loss": 0.4857,
      "step": 310
    },
    {
      "epoch": 0.46715328467153283,
      "grad_norm": 0.662590742111206,
      "learning_rate": 9.908126911489073e-05,
      "loss": 0.5064,
      "step": 320
    },
    {
      "epoch": 0.48175182481751827,
      "grad_norm": 0.6857175230979919,
      "learning_rate": 9.891208512169554e-05,
      "loss": 0.5207,
      "step": 330
    },
    {
      "epoch": 0.49635036496350365,
      "grad_norm": 0.8782424926757812,
      "learning_rate": 9.872878122748058e-05,
      "loss": 0.5195,
      "step": 340
    },
    {
      "epoch": 0.5109489051094891,
      "grad_norm": 0.8150352835655212,
      "learning_rate": 9.853141034826522e-05,
      "loss": 0.5425,
      "step": 350
    },
    {
      "epoch": 0.5255474452554745,
      "grad_norm": 0.8332362771034241,
      "learning_rate": 9.832002946091459e-05,
      "loss": 0.534,
      "step": 360
    },
    {
      "epoch": 0.5401459854014599,
      "grad_norm": 0.792351484298706,
      "learning_rate": 9.809469958669164e-05,
      "loss": 0.4862,
      "step": 370
    },
    {
      "epoch": 0.5547445255474452,
      "grad_norm": 0.9537925124168396,
      "learning_rate": 9.785548577364152e-05,
      "loss": 0.493,
      "step": 380
    },
    {
      "epoch": 0.5693430656934306,
      "grad_norm": 1.064953327178955,
      "learning_rate": 9.760245707781355e-05,
      "loss": 0.5066,
      "step": 390
    },
    {
      "epoch": 0.583941605839416,
      "grad_norm": 0.8237999081611633,
      "learning_rate": 9.733568654332618e-05,
      "loss": 0.4837,
      "step": 400
    },
    {
      "epoch": 0.5985401459854015,
      "grad_norm": 0.7828624248504639,
      "learning_rate": 9.705525118128073e-05,
      "loss": 0.4947,
      "step": 410
    },
    {
      "epoch": 0.6131386861313869,
      "grad_norm": 0.911079466342926,
      "learning_rate": 9.67612319475298e-05,
      "loss": 0.4785,
      "step": 420
    },
    {
      "epoch": 0.6277372262773723,
      "grad_norm": 0.7735104560852051,
      "learning_rate": 9.645371371930709e-05,
      "loss": 0.4984,
      "step": 430
    },
    {
      "epoch": 0.6423357664233577,
      "grad_norm": 0.8236644864082336,
      "learning_rate": 9.613278527072504e-05,
      "loss": 0.4813,
      "step": 440
    },
    {
      "epoch": 0.656934306569343,
      "grad_norm": 0.800072193145752,
      "learning_rate": 9.579853924714759e-05,
      "loss": 0.5068,
      "step": 450
    },
    {
      "epoch": 0.6715328467153284,
      "grad_norm": 0.8278487324714661,
      "learning_rate": 9.545107213844546e-05,
      "loss": 0.4626,
      "step": 460
    },
    {
      "epoch": 0.6861313868613139,
      "grad_norm": 0.8579379320144653,
      "learning_rate": 9.509048425114148e-05,
      "loss": 0.5015,
      "step": 470
    },
    {
      "epoch": 0.7007299270072993,
      "grad_norm": 0.9362450838088989,
      "learning_rate": 9.471687967945416e-05,
      "loss": 0.4784,
      "step": 480
    },
    {
      "epoch": 0.7153284671532847,
      "grad_norm": 0.9878629446029663,
      "learning_rate": 9.433036627524794e-05,
      "loss": 0.4683,
      "step": 490
    },
    {
      "epoch": 0.7299270072992701,
      "grad_norm": 0.8304235935211182,
      "learning_rate": 9.39310556168985e-05,
      "loss": 0.5026,
      "step": 500
    },
    {
      "epoch": 0.7445255474452555,
      "grad_norm": 0.8765249848365784,
      "learning_rate": 9.35190629770825e-05,
      "loss": 0.5117,
      "step": 510
    },
    {
      "epoch": 0.7591240875912408,
      "grad_norm": 0.8345662355422974,
      "learning_rate": 9.309450728950069e-05,
      "loss": 0.4673,
      "step": 520
    },
    {
      "epoch": 0.7737226277372263,
      "grad_norm": 0.8436962962150574,
      "learning_rate": 9.265751111454425e-05,
      "loss": 0.5026,
      "step": 530
    },
    {
      "epoch": 0.7883211678832117,
      "grad_norm": 0.8265005350112915,
      "learning_rate": 9.220820060391416e-05,
      "loss": 0.4496,
      "step": 540
    },
    {
      "epoch": 0.8029197080291971,
      "grad_norm": 1.0502997636795044,
      "learning_rate": 9.174670546420379e-05,
      "loss": 0.53,
      "step": 550
    },
    {
      "epoch": 0.8175182481751825,
      "grad_norm": 0.8298702836036682,
      "learning_rate": 9.12731589194554e-05,
      "loss": 0.4508,
      "step": 560
    },
    {
      "epoch": 0.8321167883211679,
      "grad_norm": 0.8837051391601562,
      "learning_rate": 9.078769767270106e-05,
      "loss": 0.4592,
      "step": 570
    },
    {
      "epoch": 0.8467153284671532,
      "grad_norm": 0.9696693420410156,
      "learning_rate": 9.029046186649927e-05,
      "loss": 0.4614,
      "step": 580
    },
    {
      "epoch": 0.8613138686131386,
      "grad_norm": 0.8426806330680847,
      "learning_rate": 8.97815950424789e-05,
      "loss": 0.4691,
      "step": 590
    },
    {
      "epoch": 0.8759124087591241,
      "grad_norm": 0.8292596340179443,
      "learning_rate": 8.926124409990149e-05,
      "loss": 0.4801,
      "step": 600
    },
    {
      "epoch": 0.8905109489051095,
      "grad_norm": 0.814528226852417,
      "learning_rate": 8.87295592532547e-05,
      "loss": 0.479,
      "step": 610
    },
    {
      "epoch": 0.9051094890510949,
      "grad_norm": 0.9417440295219421,
      "learning_rate": 8.818669398888834e-05,
      "loss": 0.4743,
      "step": 620
    },
    {
      "epoch": 0.9197080291970803,
      "grad_norm": 0.8523961305618286,
      "learning_rate": 8.763280502070617e-05,
      "loss": 0.4547,
      "step": 630
    },
    {
      "epoch": 0.9343065693430657,
      "grad_norm": 1.0132845640182495,
      "learning_rate": 8.706805224492582e-05,
      "loss": 0.4656,
      "step": 640
    },
    {
      "epoch": 0.948905109489051,
      "grad_norm": 0.888124942779541,
      "learning_rate": 8.649259869392004e-05,
      "loss": 0.474,
      "step": 650
    },
    {
      "epoch": 0.9635036496350365,
      "grad_norm": 0.8103448748588562,
      "learning_rate": 8.590661048915272e-05,
      "loss": 0.451,
      "step": 660
    },
    {
      "epoch": 0.9781021897810219,
      "grad_norm": 0.8937534093856812,
      "learning_rate": 8.531025679322302e-05,
      "loss": 0.4534,
      "step": 670
    },
    {
      "epoch": 0.9927007299270073,
      "grad_norm": 0.837407112121582,
      "learning_rate": 8.47037097610317e-05,
      "loss": 0.4627,
      "step": 680
    },
    {
      "epoch": 1.0072992700729928,
      "grad_norm": 0.8500024080276489,
      "learning_rate": 8.408714449008352e-05,
      "loss": 0.4142,
      "step": 690
    },
    {
      "epoch": 1.0218978102189782,
      "grad_norm": 1.029523491859436,
      "learning_rate": 8.346073896994031e-05,
      "loss": 0.4142,
      "step": 700
    },
    {
      "epoch": 1.0364963503649636,
      "grad_norm": 0.9407436847686768,
      "learning_rate": 8.282467403083903e-05,
      "loss": 0.4066,
      "step": 710
    },
    {
      "epoch": 1.051094890510949,
      "grad_norm": 0.8774054050445557,
      "learning_rate": 8.217913329148981e-05,
      "loss": 0.3995,
      "step": 720
    },
    {
      "epoch": 1.0656934306569343,
      "grad_norm": 0.9214247465133667,
      "learning_rate": 8.152430310606922e-05,
      "loss": 0.382,
      "step": 730
    },
    {
      "epoch": 1.0802919708029197,
      "grad_norm": 0.8537412881851196,
      "learning_rate": 8.086037251042348e-05,
      "loss": 0.406,
      "step": 740
    },
    {
      "epoch": 1.094890510948905,
      "grad_norm": 0.9219521880149841,
      "learning_rate": 8.018753316749786e-05,
      "loss": 0.3717,
      "step": 750
    },
    {
      "epoch": 1.1094890510948905,
      "grad_norm": 0.8020859360694885,
      "learning_rate": 7.95059793120076e-05,
      "loss": 0.3847,
      "step": 760
    },
    {
      "epoch": 1.1240875912408759,
      "grad_norm": 0.976387619972229,
      "learning_rate": 7.881590769436634e-05,
      "loss": 0.4179,
      "step": 770
    },
    {
      "epoch": 1.1386861313868613,
      "grad_norm": 0.8144912719726562,
      "learning_rate": 7.811751752388834e-05,
      "loss": 0.3979,
      "step": 780
    },
    {
      "epoch": 1.1532846715328466,
      "grad_norm": 0.8935455083847046,
      "learning_rate": 7.741101041128099e-05,
      "loss": 0.4092,
      "step": 790
    },
    {
      "epoch": 1.167883211678832,
      "grad_norm": 0.993914008140564,
      "learning_rate": 7.669659031044392e-05,
      "loss": 0.4286,
      "step": 800
    },
    {
      "epoch": 1.1824817518248176,
      "grad_norm": 0.9246808886528015,
      "learning_rate": 7.597446345959178e-05,
      "loss": 0.3924,
      "step": 810
    },
    {
      "epoch": 1.197080291970803,
      "grad_norm": 0.9731178283691406,
      "learning_rate": 7.524483832171756e-05,
      "loss": 0.4207,
      "step": 820
    },
    {
      "epoch": 1.2116788321167884,
      "grad_norm": 0.9542856216430664,
      "learning_rate": 7.450792552441364e-05,
      "loss": 0.4077,
      "step": 830
    },
    {
      "epoch": 1.2262773722627738,
      "grad_norm": 0.9903732538223267,
      "learning_rate": 7.376393779906806e-05,
      "loss": 0.4086,
      "step": 840
    },
    {
      "epoch": 1.2408759124087592,
      "grad_norm": 0.9735646843910217,
      "learning_rate": 7.301308991945329e-05,
      "loss": 0.4207,
      "step": 850
    },
    {
      "epoch": 1.2554744525547445,
      "grad_norm": 0.9861911535263062,
      "learning_rate": 7.225559863972555e-05,
      "loss": 0.3944,
      "step": 860
    },
    {
      "epoch": 1.27007299270073,
      "grad_norm": 0.9942032694816589,
      "learning_rate": 7.149168263185249e-05,
      "loss": 0.4058,
      "step": 870
    },
    {
      "epoch": 1.2846715328467153,
      "grad_norm": 1.0354939699172974,
      "learning_rate": 7.0721562422487e-05,
      "loss": 0.4286,
      "step": 880
    },
    {
      "epoch": 1.2992700729927007,
      "grad_norm": 1.002482533454895,
      "learning_rate": 6.994546032930582e-05,
      "loss": 0.3883,
      "step": 890
    },
    {
      "epoch": 1.313868613138686,
      "grad_norm": 0.945805013179779,
      "learning_rate": 6.916360039683109e-05,
      "loss": 0.3987,
      "step": 900
    },
    {
      "epoch": 1.3284671532846715,
      "grad_norm": 0.9589716196060181,
      "learning_rate": 6.83762083317533e-05,
      "loss": 0.3725,
      "step": 910
    },
    {
      "epoch": 1.343065693430657,
      "grad_norm": 0.9340313076972961,
      "learning_rate": 6.758351143777449e-05,
      "loss": 0.4015,
      "step": 920
    },
    {
      "epoch": 1.3576642335766422,
      "grad_norm": 0.9945915937423706,
      "learning_rate": 6.678573854999038e-05,
      "loss": 0.4252,
      "step": 930
    },
    {
      "epoch": 1.3722627737226278,
      "grad_norm": 0.9992026686668396,
      "learning_rate": 6.598311996883049e-05,
      "loss": 0.4389,
      "step": 940
    },
    {
      "epoch": 1.3868613138686132,
      "grad_norm": 0.9854737520217896,
      "learning_rate": 6.517588739357518e-05,
      "loss": 0.4146,
      "step": 950
    },
    {
      "epoch": 1.4014598540145986,
      "grad_norm": 0.9030470848083496,
      "learning_rate": 6.436427385546883e-05,
      "loss": 0.4023,
      "step": 960
    },
    {
      "epoch": 1.416058394160584,
      "grad_norm": 1.0019944906234741,
      "learning_rate": 6.354851365044868e-05,
      "loss": 0.4191,
      "step": 970
    },
    {
      "epoch": 1.4306569343065694,
      "grad_norm": 0.9242604374885559,
      "learning_rate": 6.272884227150834e-05,
      "loss": 0.3874,
      "step": 980
    },
    {
      "epoch": 1.4452554744525548,
      "grad_norm": 0.9749046564102173,
      "learning_rate": 6.1905496340716e-05,
      "loss": 0.401,
      "step": 990
    },
    {
      "epoch": 1.4598540145985401,
      "grad_norm": 0.9817001223564148,
      "learning_rate": 6.107871354090643e-05,
      "loss": 0.4028,
      "step": 1000
    },
    {
      "epoch": 1.4744525547445255,
      "grad_norm": 0.8990544676780701,
      "learning_rate": 6.024873254706701e-05,
      "loss": 0.398,
      "step": 1010
    },
    {
      "epoch": 1.489051094890511,
      "grad_norm": 1.0693354606628418,
      "learning_rate": 5.9415792957437145e-05,
      "loss": 0.3894,
      "step": 1020
    },
    {
      "epoch": 1.5036496350364965,
      "grad_norm": 1.1113970279693604,
      "learning_rate": 5.8580135224341285e-05,
      "loss": 0.4038,
      "step": 1030
    },
    {
      "epoch": 1.5182481751824817,
      "grad_norm": 0.9955109357833862,
      "learning_rate": 5.774200058477532e-05,
      "loss": 0.3859,
      "step": 1040
    },
    {
      "epoch": 1.5328467153284673,
      "grad_norm": 0.9959201216697693,
      "learning_rate": 5.690163099076653e-05,
      "loss": 0.39,
      "step": 1050
    },
    {
      "epoch": 1.5474452554744524,
      "grad_norm": 0.9857903718948364,
      "learning_rate": 5.605926903952694e-05,
      "loss": 0.3992,
      "step": 1060
    },
    {
      "epoch": 1.562043795620438,
      "grad_norm": 1.0821529626846313,
      "learning_rate": 5.521515790342073e-05,
      "loss": 0.3739,
      "step": 1070
    },
    {
      "epoch": 1.5766423357664232,
      "grad_norm": 1.0550012588500977,
      "learning_rate": 5.43695412597653e-05,
      "loss": 0.3765,
      "step": 1080
    },
    {
      "epoch": 1.5912408759124088,
      "grad_norm": 1.1277539730072021,
      "learning_rate": 5.3522663220486756e-05,
      "loss": 0.4028,
      "step": 1090
    },
    {
      "epoch": 1.6058394160583942,
      "grad_norm": 1.0444430112838745,
      "learning_rate": 5.267476826164988e-05,
      "loss": 0.3687,
      "step": 1100
    },
    {
      "epoch": 1.6204379562043796,
      "grad_norm": 1.0123330354690552,
      "learning_rate": 5.182610115288295e-05,
      "loss": 0.3885,
      "step": 1110
    },
    {
      "epoch": 1.635036496350365,
      "grad_norm": 0.972986102104187,
      "learning_rate": 5.097690688671788e-05,
      "loss": 0.3716,
      "step": 1120
    },
    {
      "epoch": 1.6496350364963503,
      "grad_norm": 1.0339381694793701,
      "learning_rate": 5.012743060786591e-05,
      "loss": 0.4241,
      "step": 1130
    },
    {
      "epoch": 1.6642335766423357,
      "grad_norm": 1.1236090660095215,
      "learning_rate": 4.927791754244955e-05,
      "loss": 0.4084,
      "step": 1140
    },
    {
      "epoch": 1.6788321167883211,
      "grad_norm": 1.0056804418563843,
      "learning_rate": 4.842861292721075e-05,
      "loss": 0.3905,
      "step": 1150
    },
    {
      "epoch": 1.6934306569343067,
      "grad_norm": 1.0442074537277222,
      "learning_rate": 4.757976193871626e-05,
      "loss": 0.3964,
      "step": 1160
    },
    {
      "epoch": 1.7080291970802919,
      "grad_norm": 1.0747994184494019,
      "learning_rate": 4.673160962258022e-05,
      "loss": 0.3884,
      "step": 1170
    },
    {
      "epoch": 1.7226277372262775,
      "grad_norm": 0.9186002612113953,
      "learning_rate": 4.588440082272466e-05,
      "loss": 0.3938,
      "step": 1180
    },
    {
      "epoch": 1.7372262773722627,
      "grad_norm": 1.0251497030258179,
      "learning_rate": 4.503838011069801e-05,
      "loss": 0.3816,
      "step": 1190
    },
    {
      "epoch": 1.7518248175182483,
      "grad_norm": 1.0191315412521362,
      "learning_rate": 4.419379171507252e-05,
      "loss": 0.3896,
      "step": 1200
    },
    {
      "epoch": 1.7664233576642334,
      "grad_norm": 1.0650057792663574,
      "learning_rate": 4.335087945094049e-05,
      "loss": 0.3856,
      "step": 1210
    },
    {
      "epoch": 1.781021897810219,
      "grad_norm": 1.0633858442306519,
      "learning_rate": 4.250988664952996e-05,
      "loss": 0.4235,
      "step": 1220
    },
    {
      "epoch": 1.7956204379562044,
      "grad_norm": 0.9760582447052002,
      "learning_rate": 4.167105608795999e-05,
      "loss": 0.3741,
      "step": 1230
    },
    {
      "epoch": 1.8102189781021898,
      "grad_norm": 1.0146124362945557,
      "learning_rate": 4.0834629919156024e-05,
      "loss": 0.3934,
      "step": 1240
    },
    {
      "epoch": 1.8248175182481752,
      "grad_norm": 0.9656704664230347,
      "learning_rate": 4.0000849601945366e-05,
      "loss": 0.3855,
      "step": 1250
    },
    {
      "epoch": 1.8394160583941606,
      "grad_norm": 0.9038411974906921,
      "learning_rate": 3.9169955831353015e-05,
      "loss": 0.3971,
      "step": 1260
    },
    {
      "epoch": 1.854014598540146,
      "grad_norm": 1.1228660345077515,
      "learning_rate": 3.834218846911804e-05,
      "loss": 0.4248,
      "step": 1270
    },
    {
      "epoch": 1.8686131386861313,
      "grad_norm": 1.0621126890182495,
      "learning_rate": 3.751778647445048e-05,
      "loss": 0.4029,
      "step": 1280
    },
    {
      "epoch": 1.883211678832117,
      "grad_norm": 1.0270401239395142,
      "learning_rate": 3.669698783504879e-05,
      "loss": 0.39,
      "step": 1290
    },
    {
      "epoch": 1.897810218978102,
      "grad_norm": 1.1217373609542847,
      "learning_rate": 3.588002949839778e-05,
      "loss": 0.4106,
      "step": 1300
    },
    {
      "epoch": 1.9124087591240877,
      "grad_norm": 1.1419464349746704,
      "learning_rate": 3.5067147303366744e-05,
      "loss": 0.3807,
      "step": 1310
    },
    {
      "epoch": 1.9270072992700729,
      "grad_norm": 1.0652042627334595,
      "learning_rate": 3.425857591212776e-05,
      "loss": 0.3803,
      "step": 1320
    },
    {
      "epoch": 1.9416058394160585,
      "grad_norm": 1.0699028968811035,
      "learning_rate": 3.345454874241352e-05,
      "loss": 0.3818,
      "step": 1330
    },
    {
      "epoch": 1.9562043795620438,
      "grad_norm": 0.9854979515075684,
      "learning_rate": 3.2655297900134585e-05,
      "loss": 0.3452,
      "step": 1340
    },
    {
      "epoch": 1.9708029197080292,
      "grad_norm": 1.099897027015686,
      "learning_rate": 3.186105411237517e-05,
      "loss": 0.4,
      "step": 1350
    },
    {
      "epoch": 1.9854014598540146,
      "grad_norm": 1.069928765296936,
      "learning_rate": 3.107204666078704e-05,
      "loss": 0.4114,
      "step": 1360
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.015566349029541,
      "learning_rate": 3.028850331540072e-05,
      "loss": 0.3661,
      "step": 1370
    },
    {
      "epoch": 2.0145985401459856,
      "grad_norm": 0.991527259349823,
      "learning_rate": 2.9510650268872962e-05,
      "loss": 0.3183,
      "step": 1380
    },
    {
      "epoch": 2.0291970802919708,
      "grad_norm": 1.0820096731185913,
      "learning_rate": 2.8738712071189676e-05,
      "loss": 0.3477,
      "step": 1390
    },
    {
      "epoch": 2.0437956204379564,
      "grad_norm": 1.0678094625473022,
      "learning_rate": 2.7972911564843007e-05,
      "loss": 0.362,
      "step": 1400
    },
    {
      "epoch": 2.0583941605839415,
      "grad_norm": 1.0742754936218262,
      "learning_rate": 2.721346982050138e-05,
      "loss": 0.3066,
      "step": 1410
    },
    {
      "epoch": 2.072992700729927,
      "grad_norm": 1.0404349565505981,
      "learning_rate": 2.646060607319091e-05,
      "loss": 0.3278,
      "step": 1420
    },
    {
      "epoch": 2.0875912408759123,
      "grad_norm": 1.0748553276062012,
      "learning_rate": 2.5714537659006887e-05,
      "loss": 0.3371,
      "step": 1430
    },
    {
      "epoch": 2.102189781021898,
      "grad_norm": 1.2123849391937256,
      "learning_rate": 2.4975479952373394e-05,
      "loss": 0.3129,
      "step": 1440
    },
    {
      "epoch": 2.116788321167883,
      "grad_norm": 1.139458179473877,
      "learning_rate": 2.4243646303869087e-05,
      "loss": 0.3711,
      "step": 1450
    },
    {
      "epoch": 2.1313868613138687,
      "grad_norm": 1.0765793323516846,
      "learning_rate": 2.3519247978637454e-05,
      "loss": 0.3304,
      "step": 1460
    },
    {
      "epoch": 2.145985401459854,
      "grad_norm": 1.0696187019348145,
      "learning_rate": 2.2802494095398828e-05,
      "loss": 0.3362,
      "step": 1470
    },
    {
      "epoch": 2.1605839416058394,
      "grad_norm": 1.082714557647705,
      "learning_rate": 2.2093591566082307e-05,
      "loss": 0.3262,
      "step": 1480
    },
    {
      "epoch": 2.1751824817518246,
      "grad_norm": 1.1737216711044312,
      "learning_rate": 2.1392745036094424e-05,
      "loss": 0.3489,
      "step": 1490
    },
    {
      "epoch": 2.18978102189781,
      "grad_norm": 1.2276502847671509,
      "learning_rate": 2.0700156825242478e-05,
      "loss": 0.3563,
      "step": 1500
    },
    {
      "epoch": 2.204379562043796,
      "grad_norm": 1.1757194995880127,
      "learning_rate": 2.001602686932879e-05,
      "loss": 0.3254,
      "step": 1510
    },
    {
      "epoch": 2.218978102189781,
      "grad_norm": 1.1644634008407593,
      "learning_rate": 1.9340552662433614e-05,
      "loss": 0.3368,
      "step": 1520
    },
    {
      "epoch": 2.2335766423357666,
      "grad_norm": 1.2510086297988892,
      "learning_rate": 1.8673929199902517e-05,
      "loss": 0.3523,
      "step": 1530
    },
    {
      "epoch": 2.2481751824817517,
      "grad_norm": 1.0367367267608643,
      "learning_rate": 1.801634892205545e-05,
      "loss": 0.3361,
      "step": 1540
    },
    {
      "epoch": 2.2627737226277373,
      "grad_norm": 1.066555380821228,
      "learning_rate": 1.7368001658633077e-05,
      "loss": 0.3097,
      "step": 1550
    },
    {
      "epoch": 2.2773722627737225,
      "grad_norm": 1.1275649070739746,
      "learning_rate": 1.672907457399701e-05,
      "loss": 0.3196,
      "step": 1560
    },
    {
      "epoch": 2.291970802919708,
      "grad_norm": 1.0739498138427734,
      "learning_rate": 1.609975211309929e-05,
      "loss": 0.3554,
      "step": 1570
    },
    {
      "epoch": 2.3065693430656933,
      "grad_norm": 1.2358207702636719,
      "learning_rate": 1.5480215948236937e-05,
      "loss": 0.3429,
      "step": 1580
    },
    {
      "epoch": 2.321167883211679,
      "grad_norm": 1.1200093030929565,
      "learning_rate": 1.487064492660703e-05,
      "loss": 0.3275,
      "step": 1590
    },
    {
      "epoch": 2.335766423357664,
      "grad_norm": 1.1512223482131958,
      "learning_rate": 1.4271215018677137e-05,
      "loss": 0.3454,
      "step": 1600
    },
    {
      "epoch": 2.3503649635036497,
      "grad_norm": 1.2044891119003296,
      "learning_rate": 1.3682099267386412e-05,
      "loss": 0.3597,
      "step": 1610
    },
    {
      "epoch": 2.3649635036496353,
      "grad_norm": 1.2894251346588135,
      "learning_rate": 1.3103467738191587e-05,
      "loss": 0.3387,
      "step": 1620
    },
    {
      "epoch": 2.3795620437956204,
      "grad_norm": 1.1263681650161743,
      "learning_rate": 1.253548746997274e-05,
      "loss": 0.3337,
      "step": 1630
    },
    {
      "epoch": 2.394160583941606,
      "grad_norm": 1.107650876045227,
      "learning_rate": 1.19783224268125e-05,
      "loss": 0.3273,
      "step": 1640
    },
    {
      "epoch": 2.408759124087591,
      "grad_norm": 1.38285231590271,
      "learning_rate": 1.1432133450663179e-05,
      "loss": 0.3301,
      "step": 1650
    },
    {
      "epoch": 2.423357664233577,
      "grad_norm": 1.1384737491607666,
      "learning_rate": 1.0897078214914925e-05,
      "loss": 0.3488,
      "step": 1660
    },
    {
      "epoch": 2.437956204379562,
      "grad_norm": 1.1954302787780762,
      "learning_rate": 1.0373311178878786e-05,
      "loss": 0.3353,
      "step": 1670
    },
    {
      "epoch": 2.4525547445255476,
      "grad_norm": 1.1164613962173462,
      "learning_rate": 9.860983543197433e-06,
      "loss": 0.329,
      "step": 1680
    },
    {
      "epoch": 2.4671532846715327,
      "grad_norm": 1.044321894645691,
      "learning_rate": 9.360243206196795e-06,
      "loss": 0.3367,
      "step": 1690
    },
    {
      "epoch": 2.4817518248175183,
      "grad_norm": 1.3004635572433472,
      "learning_rate": 8.871234721190774e-06,
      "loss": 0.3447,
      "step": 1700
    },
    {
      "epoch": 2.4963503649635035,
      "grad_norm": 1.2629735469818115,
      "learning_rate": 8.394099254751775e-06,
      "loss": 0.3315,
      "step": 1710
    },
    {
      "epoch": 2.510948905109489,
      "grad_norm": 1.201228141784668,
      "learning_rate": 7.928974545958868e-06,
      "loss": 0.3548,
      "step": 1720
    },
    {
      "epoch": 2.5255474452554747,
      "grad_norm": 1.2097376585006714,
      "learning_rate": 7.475994866635311e-06,
      "loss": 0.3117,
      "step": 1730
    },
    {
      "epoch": 2.54014598540146,
      "grad_norm": 1.3393516540527344,
      "learning_rate": 7.0352909825871614e-06,
      "loss": 0.3547,
      "step": 1740
    },
    {
      "epoch": 2.554744525547445,
      "grad_norm": 1.1093120574951172,
      "learning_rate": 6.606990115853845e-06,
      "loss": 0.3194,
      "step": 1750
    },
    {
      "epoch": 2.5693430656934306,
      "grad_norm": 1.2379064559936523,
      "learning_rate": 6.191215907981834e-06,
      "loss": 0.3348,
      "step": 1760
    },
    {
      "epoch": 2.5839416058394162,
      "grad_norm": 1.1802512407302856,
      "learning_rate": 5.788088384331852e-06,
      "loss": 0.354,
      "step": 1770
    },
    {
      "epoch": 2.5985401459854014,
      "grad_norm": 1.167688250541687,
      "learning_rate": 5.397723919430137e-06,
      "loss": 0.3402,
      "step": 1780
    },
    {
      "epoch": 2.613138686131387,
      "grad_norm": 1.0666675567626953,
      "learning_rate": 5.020235203373458e-06,
      "loss": 0.3115,
      "step": 1790
    },
    {
      "epoch": 2.627737226277372,
      "grad_norm": 1.1388198137283325,
      "learning_rate": 4.6557312092978585e-06,
      "loss": 0.3326,
      "step": 1800
    },
    {
      "epoch": 2.6423357664233578,
      "grad_norm": 1.104994297027588,
      "learning_rate": 4.304317161920424e-06,
      "loss": 0.3355,
      "step": 1810
    },
    {
      "epoch": 2.656934306569343,
      "grad_norm": 1.0453996658325195,
      "learning_rate": 3.966094507163004e-06,
      "loss": 0.3347,
      "step": 1820
    },
    {
      "epoch": 2.6715328467153285,
      "grad_norm": 1.1191107034683228,
      "learning_rate": 3.6411608828669786e-06,
      "loss": 0.3394,
      "step": 1830
    },
    {
      "epoch": 2.686131386861314,
      "grad_norm": 1.1305840015411377,
      "learning_rate": 3.3296100906071712e-06,
      "loss": 0.3109,
      "step": 1840
    },
    {
      "epoch": 2.7007299270072993,
      "grad_norm": 1.0916826725006104,
      "learning_rate": 3.0315320686133463e-06,
      "loss": 0.367,
      "step": 1850
    },
    {
      "epoch": 2.7153284671532845,
      "grad_norm": 1.085211157798767,
      "learning_rate": 2.7470128658068693e-06,
      "loss": 0.3466,
      "step": 1860
    },
    {
      "epoch": 2.72992700729927,
      "grad_norm": 1.289504051208496,
      "learning_rate": 2.4761346169602017e-06,
      "loss": 0.3342,
      "step": 1870
    },
    {
      "epoch": 2.7445255474452557,
      "grad_norm": 1.2646218538284302,
      "learning_rate": 2.218975518986277e-06,
      "loss": 0.3343,
      "step": 1880
    },
    {
      "epoch": 2.759124087591241,
      "grad_norm": 1.3023768663406372,
      "learning_rate": 1.9756098083647144e-06,
      "loss": 0.3588,
      "step": 1890
    },
    {
      "epoch": 2.7737226277372264,
      "grad_norm": 1.1600861549377441,
      "learning_rate": 1.74610773971125e-06,
      "loss": 0.3375,
      "step": 1900
    },
    {
      "epoch": 2.7883211678832116,
      "grad_norm": 1.2054556608200073,
      "learning_rate": 1.5305355654967325e-06,
      "loss": 0.3119,
      "step": 1910
    },
    {
      "epoch": 2.802919708029197,
      "grad_norm": 1.1790165901184082,
      "learning_rate": 1.3289555169213708e-06,
      "loss": 0.3305,
      "step": 1920
    },
    {
      "epoch": 2.8175182481751824,
      "grad_norm": 1.204272747039795,
      "learning_rate": 1.1414257859499344e-06,
      "loss": 0.3288,
      "step": 1930
    },
    {
      "epoch": 2.832116788321168,
      "grad_norm": 1.2812494039535522,
      "learning_rate": 9.680005085128952e-07,
      "loss": 0.3393,
      "step": 1940
    },
    {
      "epoch": 2.846715328467153,
      "grad_norm": 1.2516344785690308,
      "learning_rate": 8.087297488785339e-07,
      "loss": 0.3051,
      "step": 1950
    },
    {
      "epoch": 2.8613138686131387,
      "grad_norm": 1.0519914627075195,
      "learning_rate": 6.636594852004186e-07,
      "loss": 0.3746,
      "step": 1960
    },
    {
      "epoch": 2.875912408759124,
      "grad_norm": 1.1652307510375977,
      "learning_rate": 5.328315962444874e-07,
      "loss": 0.3315,
      "step": 1970
    },
    {
      "epoch": 2.8905109489051095,
      "grad_norm": 1.1731423139572144,
      "learning_rate": 4.162838492995036e-07,
      "loss": 0.3362,
      "step": 1980
    },
    {
      "epoch": 2.905109489051095,
      "grad_norm": 1.1167609691619873,
      "learning_rate": 3.1404988927442036e-07,
      "loss": 0.351,
      "step": 1990
    },
    {
      "epoch": 2.9197080291970803,
      "grad_norm": 1.2440118789672852,
      "learning_rate": 2.2615922898581722e-07,
      "loss": 0.3254,
      "step": 2000
    },
    {
      "epoch": 2.9343065693430654,
      "grad_norm": 1.1320172548294067,
      "learning_rate": 1.5263724063813712e-07,
      "loss": 0.3318,
      "step": 2010
    },
    {
      "epoch": 2.948905109489051,
      "grad_norm": 1.171804428100586,
      "learning_rate": 9.350514849928993e-08,
      "loss": 0.3461,
      "step": 2020
    },
    {
      "epoch": 2.9635036496350367,
      "grad_norm": 1.132089614868164,
      "learning_rate": 4.87800227736146e-08,
      "loss": 0.3303,
      "step": 2030
    },
    {
      "epoch": 2.978102189781022,
      "grad_norm": 1.1314946413040161,
      "learning_rate": 1.847477467409875e-08,
      "loss": 0.339,
      "step": 2040
    },
    {
      "epoch": 2.9927007299270074,
      "grad_norm": 1.1397558450698853,
      "learning_rate": 2.598152695160172e-09,
      "loss": 0.3404,
      "step": 2050
    },
    {
      "epoch": 3.0,
      "step": 2055,
      "total_flos": 2.416270576206217e+18,
      "train_loss": 0.09086790293672659,
      "train_runtime": 34963.1667,
      "train_samples_per_second": 0.94,
      "train_steps_per_second": 0.059
    }
  ],
  "logging_steps": 10,
  "max_steps": 2055,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.416270576206217e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
